                        Filebeat nedir?
Elasticsearch tarafÄ±ndan geliÅŸtirilmiÅŸtir, log shipper - log gÃ¶ndericidir. Filebeat genellikle sunuculardaki log dosyalarÄ±nÄ± okuyarak bu loglarÄ± Elasticsearch'e ya da Logstash'e gÃ¶nderir.
* Sistemdeki log dosyalarÄ±nÄ± takip eder(Ã¶rneÄŸin /var/log/syslog gibi)
* Yeni log satÄ±rlarÄ±nÄ± takip eder ve bunlarÄ± Elastic Stack'e aktarÄ±r.
* KaldÄ±ÄŸÄ± yeri unutmadan devam edebilir, Filebeat her log dosyasÄ±nda en son nereye kadar veri gÃ¶nderdiÄŸini registry(kayÄ±t) dosyasÄ±na yazar.
   AÄŸ kesintisi, sistem kapanmasÄ±, Elasticsearch eriÅŸilemezliÄŸi gibi durumlarda, filebeat yeniden baÅŸladÄ±ÄŸÄ±nda kaldÄ±ÄŸÄ± yerden devam eder, veri kaybÄ± olmaz.
* Harvester mantÄ±ÄŸÄ±: --> Her log dosyasÄ± iÃ§in bir harvester Ã§alÄ±ÅŸtÄ±rÄ±r. Harvester, log dosyasÄ±nÄ± okuyup gÃ¶nderen bileÅŸendir.
* diÄŸer beats tÃ¼rleri vardÄ±r: 1- Winlogbeat; Windows iÅŸletim sisteminin event loglarÄ±nÄ± toplar.
                                2- Metricbeats; Sistem kaynaklarÄ±(CPU, bellek, disk, aÄŸ vs.) gibi host metrikleri toplar.
                                3- Packetbeat; AÄŸ trafiÄŸini analiz eder, network verisini gÃ¶nderir.
                                4- Auditbeat; Linux audit framework Ã¼zerinden gÃ¼venlik olaylarÄ±nÄ± toplar.
                                5- Heartbeat; Sistemlerin eriÅŸilebilirliÄŸini(ping, http vs.) kontrol eder, uptime verisini gÃ¶nderir.
* Back pressure: EÄŸer elasticsearch veya logstash yoÄŸunluktan dolayÄ± loglarÄ± alamazsa: Filebeat kendini yavaÅŸlatÄ±r, dosya okuma hÄ±zÄ±nÄ± dÃ¼ÅŸÃ¼rerek sistemi zorlamaz.
                        Filebeat NasÄ±l Ã‡alÄ±ÅŸÄ±r?
* Filebeat yapÄ±landÄ±rma dosyasÄ±nda(filebeat.yml) tanÄ±mladÄ±ÄŸÄ±n dizin veya dosyalar iÃ§in input baÅŸlatÄ±lÄ±r. Bu inputlar, belirttiÄŸin log dosyalarÄ±nÄ± izlemeye baÅŸlar.
* Filebeat, izlenen konumda bir log dosyasÄ± bulursa: O dosya iÃ§in ayrÄ± bir harvester(toplayÄ±cÄ±) baÅŸlatÄ±r. Her harvester, sadece bir log dosyasÄ±nÄ± izler, o dosyadaki yeni eklenen satÄ±rlarÄ± sÃ¼rekli kontrol eder.
  * Harvester, okuduÄŸu yeni log satÄ±rlarÄ±nÄ± libbeat adlÄ± ara katmana gÃ¶nderir. libbeat, tÃ¼m Beats araÃ§larÄ±nda ortak olan, veri toplama ve gÃ¶nderme altyapÄ±sÄ±dÄ±r.
          harvester Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼rece, okuduÄŸu dosya aÃ§Ä±k kalÄ±r. Bu iÅŸletim sisteminde o dosyaya ait file descriptor sÃ¼rekli aktiftir demektir.
          dosya silinmiÅŸ ya da taÅŸÄ±nmÄ±ÅŸ gibi gÃ¶rÃ¼nse bile, harvester bu dosyaya eriÅŸmeye devam eder. Ã‡Ã¼nkÃ¼ iÅŸletim sistemi, dosya descriptoru aÃ§Ä±k olduÄŸu sÃ¼rece o dosyaya fiziksel olarak eriÅŸilebilir.
            Silinen bir dosya, harvester tarafÄ±ndan hÃ¢lÃ¢ okunuyorsa: Disk Ã¼zerinde kapladÄ±ÄŸÄ± alan hemen boÅŸalmaz. Ã‡Ã¼nkÃ¼ sistem dosyanÄ±n fiziksel iÃ§eriÄŸini hÃ¢lÃ¢ saklar. Filebeatâ€™in yapÄ±landÄ±rma dosyasÄ±nda bu davranÄ±ÅŸ kontrol edilebilir.
    close.on_state_change.inactive: Dosyada belirli bir sÃ¼re yeni veri gelmemiÅŸse, harvester kapanÄ±r ve dosya kapatÄ±lÄ±r.
    Harvester kapatÄ±ldÄ±ÄŸÄ±nda ne olur?  Harvester kapanÄ±nca:Dosyaya eriÅŸim saÄŸlayan file handler (dosya tanÄ±tÄ±cÄ±sÄ±) da kapatÄ±lÄ±r. EÄŸer dosya daha Ã¶nce silinmiÅŸ ama harvester aÃ§Ä±k olduÄŸu iÃ§in hÃ¢lÃ¢ sistemde tutuluyorsa: Bu alan serbest bÄ±rakÄ±lÄ±r, disk alanÄ± boÅŸalÄ±r.
  
* libbeat, gelen log kayÄ±tlarÄ±nÄ± Toplar, iÅŸler ve yapÄ±landÄ±rmada belirtilen Ã§Ä±ktÄ±ya gÃ¶nderir. Elasticsearch, Logstash, kafka vs.
*   yml dosyasÄ±nda olan ayarlara dair notlar:
    1- clean_inactive: 3h # 3 saattir deÄŸiÅŸmemiÅŸ (inaktif) log dosyalarÄ±nÄ± â€œtamamen unutsun.â€
    Ne demek "unutmak"? ilebeat normalde okuduÄŸu her log dosyasÄ±nÄ±n nerede kaldÄ±ÄŸÄ±nÄ± (offset) ve diÄŸer bilgileri .filebeat registry dosyasÄ±nda saklar.
    Ama clean_inactive: 3h dersen: Bir log dosyasÄ± 3 saat boyunca hiÃ§ yazÄ±lmamÄ±ÅŸsa, filebeat hem onu okumayÄ± bÄ±rakÄ±r hem de registry den siler. EÄŸer o dosyaya sonra yeniden yazÄ±lÄ±rsa, baÅŸtan okur, kaldÄ±ÄŸÄ±r yerden deÄŸil.
    Bu Ã¶zellik ne zaman kullanÄ±lÄ±r?  --> Ã‡ok fazla kÄ±sa Ã¶mÃ¼rlÃ¼ log dosyasÄ± varsa(Ã¶rn: gÃ¼nlÃ¼k oluÅŸa loglar) --> kaynak tÃ¼ketimini azaltmak istiyorsan(registry bÃ¼yÃ¼mesin diye)
    

    2- setup.ilm.enabled: false ve setup.template.enabled: false ne iÅŸe yarar?
       setup.template.enabled: false:  --> filebeat, elasticsearche otomatik template gÃ¶ndermesin demek. Yani log formatÄ±nÄ± sen elle tanÄ±mlayacaksÄ±n.
       setup.ilm.enabled: false: --> index lifecycle management(ilm) kapalÄ±. Yani, log verilerinini ne zaman silineceÄŸi veya taÅŸÄ±nacaÄŸÄ± gibi iÅŸlemleri sen manuel yÃ¶neteceksin.
      Bu Ã¶zellikleri ne zaman kullanmalÄ±sÄ±n? --> Daha kontrollÃ¼, Ã¶zelleÅŸtirilmiÅŸ bir yapÄ± istiyorsan. --> Kurumsal ortamda genelde Devops veya Elasticsearch adminleri, bu ayarlarÄ± elle yapar.

---- docker compose yml dosyasÄ±nÄ± kurduktan sonra, her ÅŸeyin yolunda olduÄŸunu anlamak iÃ§in ÅŸu kontrolleri yapmalÄ±sÄ±n:
1) Filebeat Logstashâ€™e baÄŸlandÄ± mÄ±?
   docker compose logs --tail=120 filebeat


2) Logstash pipeline 5044â€™Ã¼ dinliyor mu?
   docker compose logs --tail=200 logstash



3) Elasticsearchâ€™te index oluÅŸtu mu?

komutumuz ÅŸu:

curl -k -u "elastic:${ELASTIC_PASSWORD}" 'https://localhost:9200/_cat/indices?v'

ÅŸifreli hali ise ÅŸu ÅŸekilde olacak: ben deneme amaÃ§lÄ± bir ÅŸifre vereyim buraya:

curl -k -u "elastic:elastic123" 'https://localhost:9200/_cat/indices?v'


4) Kibanaâ€™da gÃ¶rselleÅŸtirme

Kibana â†’ http://localhost:5601

------- proje kapsamÄ± --> gÃ¼ncel elastic search sÃ¼rÃ¼mÃ¼nÃ¼ yÃ¼kleyeceÄŸim
1- elastic search iÃ§in bir node yapacaÄŸÄ±m replicalara girmeyeceÄŸim shardlara girmeyeceÄŸim.
    loglarÄ± gÃ¼nlÃ¼k tutacaÄŸÄ±m, 7 gÃ¼nden sonraki loglarÄ± otomatik sildireceÄŸim
    tek node elastic kuracaÄŸÄ±m, filebeay ve logstah ayakta iken elastic in Ã§Ã¶kmesi ve yeniden ayaÄŸa kalkana kdardki loglarÄ± kaybetmemsi Ã¼zerine kurgu
2- gÃ¼venlik katmanÄ± ekleyeceÄŸim

3- APM nedir? kibana veya elastic Ã¼zerinden bunu nasÄ±l entegre edebilirim. SErvice Map yaopÄ±sÄ±nÄ± nasÄ±l bulabilirim, bÃ¼tÃ¼n servislerin birbirini hangi sÄ±rada Ã§aÄŸÄ±rdÄ±ÄŸÄ±n bilgisini gÃ¶rebiliyorum

front end veya backend agentlar vasÄ±tasÄ± ile Elastic APM servera veri gÃ¶nderir --> elastic search -> kibana

 --> cursore kod yazdÄ±rÄ±rken isteyeceklerim:
 1- filebeat iÃ§in prod ortama uyumlu olmasÄ± adÄ±na chatgpt ÅŸunlarÄ± Ã¶nerdi:
   * TLS / Authentication (GÃ¼venlik)
     output.logstash:
     ssl.enabled: true
     ssl.certificate_authorities: ["/etc/filebeat/certs/ca.crt"]
     ssl.certificate: "/etc/filebeat/certs/filebeat.crt"
     ssl.key: "/etc/filebeat/certs/filebeat.key"

        Neden Ã¶nemli? prod ortamda loglarÄ±n logstashe ÅŸifrelenmiÅŸ ve kimlik doÄŸrulamalÄ± gitmesi gerekir. Aksi takdirde loglar  aÃ§Ä±k aÄŸda dÃ¼z metin gidebilir.

    * ğŸ“› Log KaynaÄŸÄ± Etiketleme (fields)
           fields:
                env: production
                service: spring-boot-app
                fields_under_root: true 

        Neden Ã¶nemli? Kibana'da filtreleme, dashboard ve alarm kurmak iÃ§in Ã§ok kullanÄ±ÅŸlÄ± olur.

     * ğŸ“Š Monitoring / Metric Output (isteÄŸe baÄŸlÄ±)   --> bu madddeyi Ã§ok anlamadÄ±m?
      Prodâ€™da merkezi log yÃ¶netimi yanÄ±nda Filebeatâ€™in kendisini izlemek de Ã¶nemlidir.
        monitoring:
            enabled: true
            elasticsearch:
            hosts: ["https://elasticsearch:9200"]
            username: "beats_system"
            password: "your-password"

        * 4. ğŸ“ Volume ve Path Uyum KontrolÃ¼  --> bunun da Ã§ok anlamadÄ±m
                EÄŸer Filebeat bir container iÃ§inde Ã§alÄ±ÅŸÄ±yorsa, paths: /logs/spring-boot-app*.log kÄ±smÄ±nda:
                Host veya container iÃ§inde doÄŸru volume mount yapÄ±lmalÄ±.
                
                Ã–rn:
                
                bash
                Copy
                Edit
                docker run -v /var/logs:/logs ...


                        Logstash
![img.png](img.png)

* logstash, log dosyalarÄ±na ek olarak xml, html ve csv gibi dosyalarÄ± gerÃ§ek zamanlÄ± olarak iÅŸleyebilir. AynÄ± zamanda veri manipÃ¼lasyonu yapÄ±larak yalnÄ±zca gerekli veriler ile Ã§alÄ±ÅŸÄ±labilir. 
* log stahsda filteleme yapÄ±lÄ±r, bu aÅŸamada veri manipÃ¼lasyonu yapÄ±lÄ±r. Ä°stenmeyen bazÄ± parametrelerin silinmesi, filtreleme, bazÄ± deÄŸerlerin veya formatÄ±n deÄŸiÅŸtirilmesi gibi iÅŸlemler uygulanabilir.
* logstash kullanarak beats, rabbitmp ve hatta twitter gibi birÃ§ok farklÄ± konumdan loglarÄ± toplayabilir, filter plug-in ve regexler kullanarak loglarÄ± parse layabilir. 
   Ã–rneÄŸin, geoip filter plug-in kullanarak ERROR loglarÄ±nÄ±n harita Ã¼zerinde en Ã§ok nerelerden geldiÄŸini gÃ¶rebilirsiniz ve sonrasÄ±nda bu loglarÄ± Elasticsearch, sentry gibi log aggregatorlara
    gÃ¶nderebilir ya da herhangi bir dosyaya yazabilirsiniz.
* Ã¼cretsiz ve aÃ§Ä±k kaynaklÄ±dÄ±r. GerÃ§ek zamanlÄ± olarak iÅŸler.
* Logstash, farklÄ±(uyumsuz) kaynaklardan gelen verileri dinmaik ÅŸekilde birleÅŸtirebilir ve bunlarÄ± seÃ§tiÄŸiniz hedef sistemlere normalize ederek aktarabilir.
   yani; JSON, syslog, CSV,Apache loglarÄ± gibi farklÄ± formatlardaki verileri alabilir. TÃ¼m bu farklÄ± verileri ortak bir yapÄ±ya(Ã¶rneÄŸin Elasticsearche uygun hale) getirir.
         ArdÄ±ndan bu verileri Elasticsearch, dosya, Kafka gibi sistemlere gÃ¶nderir.
* Verilerdeki gereksiz alanlar silinebilir, eksik bilgiler tamamlanabilir. BÃ¶ylece temiz ve anlamlÄ± veri, Kibana gibi araÃ§lara kolayca analiz edilir hale gelir.
* Logstash veri iÅŸleme hattÄ± Ã¼Ã§ bÃ¶lÃ¼mden oluÅŸur: GiriÅŸ (Input), Filtre (Filter) ve Ã‡Ä±kÄ±ÅŸ (Output).
  ![img_1.png](img_1.png) 
    * Input Stage: logstash, her tÃ¼rden ve kaynaktan veri almak iÃ§in farklÄ± giriÅŸ eklentileri kullanÄ±r. Bu eklentiler sayesinde veriler Ã§eÅŸitli sistemlerden Ã§ekilebilir.
    * Filter Stage: logstash, filtre eklentilerini kullanarak gelen veriden gerekli bilgileri Ã§Ä±karÄ±r ve bunlarÄ± daha anlamlÄ± ve standart bir formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
            farklÄ± iÅŸleme ihtiyaÃ§larÄ± iÃ§in farklÄ± filtre eklentileri vardÄ±r.-> JSON: json verileri ayrÄ±ÅŸtÄ±rÄ±r. --> xml: xml formatÄ±ndaki verileri iÅŸler. ...GeoIp: ip adresinden Ã¼lke, ÅŸehir, konum gibi bilgiler Ã§Ä±karÄ±r.
    * Output stage: GiriÅŸ aÅŸamasÄ±nda veri nasÄ±l alÄ±nÄ±yorsa, Ã§Ä±kÄ±ÅŸ aÅŸamasÄ±nda da logstash, toplanmÄ±ÅŸ ve temizlenmiÅŸ veriyi belirli hedeflere gÃ¶nderir. Ã¶rnek Elasticsearch, AWS s3, 
                    Logstash sadece tek bir yere deÄŸil, Birden fazla hedefe veri gÃ¶ndermeni saÄŸlar. AyrÄ±ca kendi yÃ¶nlendirme kurallarÄ±nÄ± yazabilirsin. Ã–rnek: HatalÄ± loglar (level: error) --> bir Elasticsearch indexine, normal loglar(level info) baÅŸka bir indexe veya s3 e

   * Logstash'in 200 den fazla eklenti iÃ§eren yapÄ±sÄ± sayesinde, veri alma, iÅŸleme ve gÃ¶nderme aÅŸamalarÄ±nÄ± kendi ihtiyaÃ§larÄ±na gÃ¶re Ã¶zgÃ¼rce tasarlayabilirsin. Bu da logstashi modÃ¼ler bir yapÄ± olduÄŸunu gÃ¶steriyor.
        Input, Filter ve output gibi her bÃ¶lÃ¼mde farklÄ± eklentiler kullanÄ±labilir. Bu parÃ§alarÄ± istediÄŸin gibi birleÅŸtirip veri iÅŸleme sÃ¼recini Ã¶zelleÅŸtirebilirsin.
        Ã–rnek: input: Filebeat --> TCP --> Kafka
               filter: JSON --> mutate  --> geoip
               output : Elasticsearch + S3(aynÄ± anda iki yere)

    * Veriler farklÄ± ÅŸekillerde geldiÄŸinde, analiz edilebilir hale getirmek Ã§ok zordur. EÄŸer veriler aynÄ± yapÄ±da deÄŸilse, onlarÄ± karÅŸÄ±laÅŸtÄ±rmak, anlamlÄ± sonuÃ§lar Ã§Ä±karmak ve etkileÅŸimleri gÃ¶rmek
        oldukÃ§a gÃ¼Ã§tÃ¼r.Ä°ÅŸte bu yÃ¼zden logstash gibi araÃ§lar, verileri tek tip ve analiz edilebilir bir yapÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rmede kritik bir rol oynar. GÃ¼nÃ¼mÃ¼zde veriler Ã§ok farklÄ± kaynaklardan ve farklÄ± formatalarda geliyor.
            Bu durum veriyi etkili bir ÅŸekilde kullanmayÄ± zorlaÅŸtÄ±rÄ±yor. Gelebilecek farklÄ± formatlardaki verilere Ã¶rnek verelim. YapÄ±landÄ±rÄ±lmÄ±ÅŸ veriler(Tablo gibi dÃ¼zenli, Ã¶rneÄŸin veritabanÄ± kayÄ±tlar)
            Yar yapÄ±landÄ±rÄ±lmÄ±ÅŸ(json,xml gibi belli bir dÃ¼zen var ama sabit deÄŸil). YapÄ±sÄ±z(serbest metin - e-postalar, PDF, loglar)
                Neden sorun oluyor, --> Analiz motorlarÄ± Ã¶rneÄŸin elasticsearch, spark bu verileri doÄŸrudan iÅŸleyemez Ã§Ã¼nkÃ¼ formatlar tutarsÄ±z olabilir. FarklÄ± formatlara sahip veri kÃ¼melerini: * karÅŸÄ±laÅŸtÄ±rmak, * birleÅŸtirmek, * Analiz etmek Ã§ok zordur.
  * Filtreleme, logstashin harika Ã¶zelliklerinden biridir. Ã‡eÅŸitli filtre seÃ§eneklerini kullanarak, loglardan veri almak iÃ§in ince ayarlar yapabilirsiniz.
          Filtreleme iÃ§in bir kaÃ§ yok var:
    1- grok; 2- date; 3- key=value;  4- multiline;  5-mutate.
                 1- Grok Filter
         --> Grok filtresi, metinleri Ã¶n tanÄ±mlÄ± desenlerle eÅŸleÅŸtirerek veri alanlarÄ±na ayÄ±rmanÄ± saÄŸlar. Ã–zellikle log dosyalarÄ±nÄ±n parÃ§alanÄ±p anlamlÄ± parÃ§alara ayrÄ±lmasÄ± iÃ§in
             kullanÄ±lÄ±r.
          ğŸ’¡ Temel KullanÄ±mÄ±
                 filter {
                 grok {
                 match => { "message" => "%{COMMONAPACHELOG}" }
                 }
                 }
            
      Bu Ã¶rnekte: message: Girdi log satÄ±rÄ±. %{COMMONAPACHELOG}: Grok tarafÄ±ndan saÄŸlanan hazÄ±r bir desen (Apache log formatÄ±)
        
            2- Date Filter
         --> Logstash, her aldÄ±ÄŸÄ± olaya otomatik olarak bir @timestamp (olayÄ±n iÅŸlendiÄŸi zaman) deÄŸeri atar. Ama bu, logun gerÃ§ek oluÅŸtuÄŸu zamanÄ± deÄŸil,
            sadece Logstash in  olayÄ± aldÄ±ÄŸÄ± zamanÄ± gÃ¶sterir. EÄŸer logun iÃ§inde gerÃ§ek zaman bilgisi varsa, bunu yakalayarak gerÃ§ek olay zamanÄ±nÄ± @timestamp
           alanÄ±na yazmak iÃ§n date filtresi kullanÄ±lÄ±r.
          
            Temel kullanÄ±mÄ±

                  filter {
                  date {
                  match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
                  target => "@timestamp"
                  }
                  }
              yukarÄ±daki Ã¶rnekte; "timestamp": Daha Ã¶nce grok filtresi ile ayrÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ zaman alanÄ±.  / "dd/MMM/yyyy:HH:mm:ss Z": Logâ€™un iÃ§indeki zamanÄ±n biÃ§imi (Ã¶rneÄŸin 06/Aug/2025:10:00:00 +0000)
                  / @timestamp: Logstashâ€™in zaman alanÄ±; buraya yazÄ±lÄ±r.
         
            
               Ã–rnek AkÄ±ÅŸ
          log
          message: 127.0.0.1 - - [06/Aug/2025:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 2326

         logstash

             filter {
             grok {
             match => { "message" => "%{IP} - - \[%{HTTPDATE:timestamp}\] ..." }
             }
             
             date {
             match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
             target => "@timestamp"
             }
             }
         
         -> grok filtresi ile logtaki tarih timestamp alanÄ±na Ã§Ä±karÄ±lÄ±r. --> date filtresi bu timestamp alanÄ±nÄ± alÄ±r ve @timestamp olarak atar.

        diÄŸer filtreleme iÃ§in araÅŸtracaÄŸÄ±m sonrasÄ±nda
  
